import os
import re
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models

MODEL_PATH = "model.keras"
EPOCH = 5  # –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è/–¥–æ–æ–±—É—á–µ–Ω–∏—è

def sanitize_name(name):
    """
    –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ '_', –Ω–∞ '_'
    """
    return re.sub(r'[^0-9a-zA-Z_]', '_', name)

def load_csv_clean(file_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV, –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–æ–ª–±—Ü—ã —Ç–∏–ø–æ–≤ number, object, bool.
    –ü—Ä–∏–≤–æ–¥–∏—Ç object –∏ bool –∫ —Å—Ç—Ä–æ–∫–µ, –∞ –ø–æ—Ç–æ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ—Ç —Å—Ç–æ–ª–±—Ü—ã —á–µ—Ä–µ–∑ sanitize.
    """
    df = pd.read_csv(file_path)
    df = df.select_dtypes(include=["number", "object", "bool"])

    # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ object/bool –∫–æ–ª–æ–Ω–∫–∏ –∫ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é
    for col in df.select_dtypes(include=["object", "bool"]).columns:
        df[col] = df[col].astype(str).fillna("missing")
    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É (–µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Ç–∞–º —Å—Ç—Ä–æ–∫–∏)
    for col in df.select_dtypes(include=["int64", "float64"]).columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö (–≤ –æ–±—É—á–∞—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —Ç–∞–∫–∏—Ö –±—ã—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–æ)
    df = df.dropna(subset=df.select_dtypes(include=["int64", "float64"]).columns)
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Å—Ç–æ–ª–±—Ü—ã –≤ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–µ–Ω–∞
    new_columns = {col: sanitize_name(col) for col in df.columns}
    df = df.rename(columns=new_columns)
    return df

def build_preprocessor_model(df):
    """
    –°—Ç—Ä–æ–∏—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å-–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫.
    –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ ‚Äì –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è;
    –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö ‚Äì StringLookup + CategoryEncoding.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - inputs: —Å–ª–æ–≤–∞—Ä—å Keras Input-—Å–ª–æ—ë–≤ (–∫–ª—é—á–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –∏–º–µ–Ω–∞–º–∏ –∏–∑ DataFrame)
      - concatenated: –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –≤—Ö–æ–¥–æ–º –¥–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞
    """
    inputs = {}
    processed_features = []
    
    # –ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
    for col in numeric_cols:
        inp = layers.Input(shape=(1,), name=col)
        norm = layers.Normalization(name=f"{col}_norm")
        norm.adapt(df[col].values.reshape(-1, 1))
        processed = norm(inp)
        inputs[col] = inp
        processed_features.append(processed)
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    categorical_cols = df.select_dtypes(include=["object"]).columns.tolist()
    for col in categorical_cols:
        inp = layers.Input(shape=(1,), name=col, dtype=tf.string)
        lookup = layers.StringLookup(name=f"{col}_lookup")
        lookup.adapt(df[col].values)
        encoded = lookup(inp)
        num_tokens = lookup.vocabulary_size()
        onehot = layers.CategoryEncoding(num_tokens=num_tokens, output_mode="one_hot", name=f"{col}_onehot")
        processed = onehot(encoded)
        inputs[col] = inp
        processed_features.append(processed)
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Äì –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö
    if len(processed_features) > 1:
        concatenated = layers.concatenate(processed_features, name="concatenated_features")
    else:
        concatenated = processed_features[0]
    
    return inputs, concatenated

def build_autoencoder_model(input_dim, encoding_dim=32):
    """
    –°—Ç—Ä–æ–∏—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏–π –Ω–∞ –≤—Ö–æ–¥ –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ input_dim.
    """
    ae_input = layers.Input(shape=(input_dim,), name="ae_input")
    encoded = layers.Dense(encoding_dim, activation="relu")(ae_input)
    decoded = layers.Dense(input_dim, activation="sigmoid")(encoded)
    autoencoder = models.Model(inputs=ae_input, outputs=decoded, name="autoencoder")
    autoencoder.compile(optimizer="adam", loss="mse")
    return autoencoder

def build_full_model(df, encoding_dim=32):
    """
    –°—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–æ–º –∏ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º.
    –í—Ö–æ–¥ ‚Äì —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏; –Ω–∞ –≤—ã—Ö–æ–¥–µ ‚Äì —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞.
    """
    inputs, preprocessed = build_preprocessor_model(df)
    input_dim = preprocessed.shape[-1]
    autoencoder = build_autoencoder_model(input_dim, encoding_dim)
    
    reconstruction = autoencoder(preprocessed)
    full_model = models.Model(inputs=inputs, outputs=reconstruction, name="full_autoencoder")
    full_model.compile(optimizer="adam", loss="mse")
    return full_model

def df_to_model_inputs(df, input_keys):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DataFrame –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å.
    –î–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤ —Å —Ç–∏–ø–æ–º object —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä —Å dtype=tf.string,
    –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö ‚Äì —Ç–µ–Ω–∑–æ—Ä —Å dtype=tf.float32.
    """
    data = {}
    for key in input_keys:
        col = df[key]
        if col.dtype == "object":
            arr = tf.convert_to_tensor(col.astype(str).tolist(), dtype=tf.string)
            arr = tf.reshape(arr, (-1, 1))
        else:
            arr = tf.convert_to_tensor(col.to_numpy().astype(np.float32), dtype=tf.float32)
            arr = tf.reshape(arr, (-1, 1))
        data[key] = arr
    return data

def train_model_on_files(file_list, epochs=EPOCH):
    """
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Å–ø–∏—Å–∫–µ —Ñ–∞–π–ª–æ–≤.
    """
    all_dfs = [load_csv_clean(f) for f in file_list]
    df = pd.concat(all_dfs, ignore_index=True)
    
    full_model = build_full_model(df, encoding_dim=32)
    input_keys = list(full_model.input.keys())
    X = df_to_model_inputs(df, input_keys)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –ø–æ–¥—Å–µ—Ç—å –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞
    inputs, preprocessed = build_preprocessor_model(df)
    preprocessor_model = models.Model(inputs=inputs, outputs=preprocessed, name="preprocessor")
    y = preprocessor_model.predict(X)
    
    full_model.fit(X, y, epochs=epochs, batch_size=32, shuffle=True, verbose=1)
    full_model.save(MODEL_PATH)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

def continue_training(file_list, epochs=EPOCH):
    """
    –î–æ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")
    
    full_model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    
    all_dfs = [load_csv_clean(f) for f in file_list]
    df = pd.concat(all_dfs, ignore_index=True)
    input_keys = list(full_model.input.keys())
    X = df_to_model_inputs(df, input_keys)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ —É—Ä–æ–≤–µ–Ω—å "concatenated_features"
    preprocessor_model = models.Model(inputs=full_model.input,
                                      outputs=full_model.get_layer("concatenated_features").output,
                                      name="preprocessor")
    y = preprocessor_model.predict(X)
    
    full_model.fit(X, y, epochs=epochs, batch_size=32, shuffle=True, verbose=1)
    full_model.save(MODEL_PATH)
    print(f"\nüîÑ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")

def validate_file_with_autoencoder(file_path, threshold=0.01):
    """
    –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π —Ñ–∞–π–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
    –°—Ç—Ä–æ–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –æ—à–∏–±–∫–æ–π –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–º–µ—á–∞—é—Ç—Å—è –∫–∞–∫ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ.
    """
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    
    full_model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    df = load_csv_clean(file_path)
    
    input_keys = list(full_model.input.keys())
    X = df_to_model_inputs(df, input_keys)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ —É—Ä–æ–≤–µ–Ω—å "concatenated_features"
    preprocessor_model = models.Model(inputs=full_model.input,
                                      outputs=full_model.get_layer("concatenated_features").output,
                                      name="preprocessor")
    y_true = preprocessor_model.predict(X)
    
    # –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
    y_pred = full_model.predict(X)
    loss = np.mean(np.square(y_true - y_pred), axis=1)
    
    # –ü–æ–º–µ—á–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ—à–∏–±–∫–æ–π –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
    df["ValidationLoss"] = loss
    df["IsValid"] = df["ValidationLoss"] < threshold
    
    print(df[["ValidationLoss", "IsValid"]].head())
    print(f"\n‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {(df['IsValid']).sum()} –∏–∑ {len(df)}")
    
    return df

# === –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ===
if __name__ == "__main__":
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
    # train_model_on_files(["data1.csv", "data2.csv"])
    
    # –î–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
    # continue_training(["data3.csv"])
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ (–Ω–∞ –≤—Ö–æ–¥ –º–æ–≥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å—Å—è –ª—é–±—ã–µ –¥–∞–Ω–Ω—ã–µ):
    # validate_file_with_autoencoder("unseen_data.csv", threshold=0.02)
    pass
