import os
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from tensorflow.keras import layers, models

MODEL_PATH = "model.h5"
PREPROCESSOR_PATH = "preprocessor.pkl"


def load_csv_clean(file_path):
    df = pd.read_csv(file_path)
    df = df.select_dtypes(include=["number", "object", "bool"])

    # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ object/bool –∫–æ–ª–æ–Ω–∫–∏ –∫ —Å—Ç—Ä–æ–∫–∞–º
    for col in df.select_dtypes(include=["object", "bool"]).columns:
        df[col] = df[col].astype(str).fillna("missing")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–µ—Å–ª–∏ –æ–Ω–∏ —Å—Ç—Ä–æ–∫–∞–º–∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã)
    for col in df.select_dtypes(include=["int64", "float64"]).columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df = df.dropna()
    return df


def build_preprocessor(df):
    numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns.tolist()
    categorical_cols = df.select_dtypes(include=["object"]).columns.tolist()

    preprocessor = ColumnTransformer([
        ("num", Pipeline([
            ("imputer", SimpleImputer(strategy="mean")),
            ("scaler", StandardScaler())
        ]), numeric_cols),
        ("cat", Pipeline([
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("encoder", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
        ]), categorical_cols)
    ])
    return preprocessor


def build_autoencoder(input_dim, encoding_dim=32):
    input_layer = layers.Input(shape=(input_dim,))
    encoded = layers.Dense(encoding_dim, activation="relu")(input_layer)
    decoded = layers.Dense(input_dim, activation="sigmoid")(encoded)
    model = models.Model(inputs=input_layer, outputs=decoded)
    model.compile(optimizer="adam", loss="mse")
    return model


def train_model_on_files(file_list, epochs=20):
    all_dfs = [load_csv_clean(f) for f in file_list]
    df = pd.concat(all_dfs, ignore_index=True)

    preprocessor = build_preprocessor(df)
    X = preprocessor.fit_transform(df)

    model = build_autoencoder(X.shape[1])
    model.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, verbose=1)

    joblib.dump(preprocessor, PREPROCESSOR_PATH)
    model.save(MODEL_PATH)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")
    print(f"‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {PREPROCESSOR_PATH}")


def continue_training(file_list, epochs=10):
    if not os.path.exists(MODEL_PATH) or not os.path.exists(PREPROCESSOR_PATH):
        raise FileNotFoundError("–ú–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")

    model = tf.keras.models.load_model(MODEL_PATH)
    preprocessor = joblib.load(PREPROCESSOR_PATH)

    all_dfs = [load_csv_clean(f) for f in file_list]
    df = pd.concat(all_dfs, ignore_index=True)
    X = preprocessor.transform(df)

    model.fit(X, X, epochs=epochs, batch_size=32, shuffle=True, verbose=1)
    model.save(MODEL_PATH)
    print(f"\nüîÑ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")


def validate_file(file_path, threshold=0.01):
    if not os.path.exists(MODEL_PATH) or not os.path.exists(PREPROCESSOR_PATH):
        raise FileNotFoundError("–ú–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

    model = tf.keras.models.load_model(MODEL_PATH)
    preprocessor = joblib.load(PREPROCESSOR_PATH)

    df = load_csv_clean(file_path)
    X = preprocessor.transform(df)
    reconstructed = model.predict(X)
    loss = np.mean(np.square(X - reconstructed), axis=1)

    df["ValidationLoss"] = loss
    df["IsValid"] = df["ValidationLoss"] < threshold

    print(df[["ValidationLoss", "IsValid"]].head())
    print(f"\n‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {(df['IsValid']).sum()} –∏–∑ {len(df)}")

    return df


# === –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ===
if __name__ == "__main__":
    # === –û–ë–£–ß–ï–ù–ò–ï –° –ù–£–õ–Ø ===
    # train_model_on_files(["data1.csv", "data2.csv"])

    # === –î–ûO–ë–£–ß–ï–ù–ò–ï –ù–ê –ù–û–í–´–• –î–ê–ù–ù–´–• ===
    # continue_training(["data3.csv"])

    # === –ü–†–û–í–ï–†–ö–ê –ù–û–í–û–ì–û –§–ê–ô–õ–ê ===
    # validate_file("unseen_data.csv", threshold=0.02)
    pass
