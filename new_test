import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models

MODEL_PATH = "model.h5"

def load_csv_clean(file_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV, –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç–æ–ª–±—Ü—ã —Ç–∏–ø–æ–≤ number, object, bool.
    –î–ª—è object –∏ bool –ø—Ä–∏–≤–æ–¥–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –∫ —Å—Ç—Ä–æ–∫–µ.
    """
    df = pd.read_csv(file_path)
    df = df.select_dtypes(include=["number", "object", "bool"])
    
    for col in df.select_dtypes(include=["object", "bool"]).columns:
        df[col] = df[col].astype(str).fillna("missing")
    # –ï—Å–ª–∏ –≤ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Å—Ç—Ä–æ–∫–∏, –∏—Ö –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —á–∏—Å–ª—É;
    # –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è ‚Äì –æ—Å—Ç–∞–≤–∏–º –∫–∞–∫ –µ—Å—Ç—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏.
    for col in df.select_dtypes(include=["int64", "float64"]).columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    df = df.dropna()  # –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ –±–µ–∑ NaN
    return df

def build_preprocessor_model(df):
    """
    –°—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª—å-–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫, –∫–æ—Ç–æ—Ä–∞—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—Ç–æ–ª–±—Ü–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä.
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–æ–≤ pandas.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º:
      - inputs: —Å–ª–æ–≤–∞—Ä—å Keras Input-—Å–ª–æ—ë–≤ (–∫–ª—é—á ‚Äì –∏–º—è –∫–æ–ª–æ–Ω–∫–∏)
      - outputs: –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä (—á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã)
    """
    inputs = {}
    processed_features = []
    
    # –ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
    for col in numeric_cols:
        inp = layers.Input(shape=(1,), name=col)
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä; –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º –Ω–∞ –∑–Ω–∞—á–µ–Ω–∏—è—Ö —ç—Ç–æ–π –∫–æ–ª–æ–Ω–∫–∏
        norm = layers.Normalization(name=f"{col}_norm")
        # –î–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ –≤–∏–¥–µ –º–∞—Å—Å–∏–≤–∞ (2D)
        norm.adapt(df[col].values.reshape(-1, 1))
        processed = norm(inp)
        inputs[col] = inp
        processed_features.append(processed)
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤–∫–ª—é—á–∞—è –±—É–ª–µ–≤–æ–µ –∏ –¥–∞—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏)
    categorical_cols = df.select_dtypes(include=["object"]).columns.tolist()
    for col in categorical_cols:
        inp = layers.Input(shape=(1,), name=col, dtype=tf.string)
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–π –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—Ç—Ä–æ–∫–∏ –≤ –∏–Ω–¥–µ–∫—Å
        lookup = layers.StringLookup(name=f"{col}_lookup")
        lookup.adapt(df[col].values)
        encoded = lookup(inp)
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è; –¥–æ–±–∞–≤–ª—è–µ–º +1, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)
        num_tokens = lookup.vocabulary_size()
        # One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        onehot = layers.CategoryEncoding(num_tokens=num_tokens, output_mode="one_hot", name=f"{col}_onehot")
        processed = onehot(encoded)
        inputs[col] = inp
        processed_features.append(processed)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –µ–¥–∏–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    if len(processed_features) > 1:
        concatenated = layers.concatenate(processed_features, name="concatenated_features")
    else:
        concatenated = processed_features[0]
    
    return inputs, concatenated

def build_autoencoder_model(input_dim, encoding_dim=32):
    """
    –°—Ç—Ä–æ–∏—Ç –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ input_dim –∏ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –µ–≥–æ.
    """
    ae_input = layers.Input(shape=(input_dim,), name="ae_input")
    encoded = layers.Dense(encoding_dim, activation="relu")(ae_input)
    decoded = layers.Dense(input_dim, activation="sigmoid")(encoded)
    autoencoder = models.Model(inputs=ae_input, outputs=decoded, name="autoencoder")
    autoencoder.compile(optimizer="adam", loss="mse")
    return autoencoder

def build_full_model(df, encoding_dim=32):
    """
    –°—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å, –≤ –∫–æ—Ç–æ—Ä–æ–π:
      - –í—Ö–æ–¥—ã ‚Äì —ç—Ç–æ –¥–∞–Ω–Ω—ã–µ (—Å–ª–æ–≤–∞—Ä—å, –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º)
      - –ß–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å-–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
      - –≠—Ç–æ—Ç –≤–µ–∫—Ç–æ—Ä –ø–æ–¥–∞—ë—Ç—Å—è –Ω–∞ –≤—Ö–æ–¥ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞
      - –ù–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
    –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –±—É–¥–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    inputs, preprocessed = build_preprocessor_model(df)
    # –û–ø—Ä–µ–¥–µ–ª–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
    input_dim = preprocessed.shape[-1]
    autoencoder = build_autoencoder_model(input_dim, encoding_dim)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
    reconstruction = autoencoder(preprocessed)
    
    # –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å: –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—ë—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å, –∞ –≤—ã—Ö–æ–¥ ‚Äì —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
    full_model = models.Model(inputs=inputs, outputs=reconstruction, name="full_autoencoder")
    full_model.compile(optimizer="adam", loss="mse")
    return full_model

def df_to_model_inputs(df, input_keys):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DataFrame –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å.
    input_keys ‚Äì —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫, –æ–∂–∏–¥–∞–µ–º—ã—Ö –º–æ–¥–µ–ª—å—é.
    """
    data = {}
    for key in input_keys:
        # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç —Ñ–æ—Ä–º—É (n,1)
        data[key] = df[key].values.reshape(-1, 1)
    return data

def train_model_on_files(file_list, epochs=20):
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    all_dfs = [load_csv_clean(f) for f in file_list]
    df = pd.concat(all_dfs, ignore_index=True)
    
    # –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–æ–º
    full_model = build_full_model(df, encoding_dim=32)
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö –∫–ª—é—á–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏
    input_keys = list(full_model.input.keys())
    X = df_to_model_inputs(df, input_keys)
    
    # –î–ª—è –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏,
    # –ø–æ—ç—Ç–æ–º—É —Å–¥–µ–ª–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–æ–≥–æ.
    # –ó–¥–µ—Å—å –º—ã –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –ø–æ–¥—Å–µ—Ç—å.
    inputs, preprocessed = build_preprocessor_model(df)
    preprocessor_model = models.Model(inputs=inputs, outputs=preprocessed, name="preprocessor")
    y = preprocessor_model.predict(X)
    
    full_model.fit(X, y, epochs=epochs, batch_size=32, shuffle=True, verbose=1)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å—é –º–æ–¥–µ–ª—å, –≤–∫–ª—é—á–∞—è —Å–ª–æ–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞
    full_model.save(MODEL_PATH)
    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

def continue_training(file_list, epochs=10):
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")
    
    full_model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    
    all_dfs = [load_csv_clean(f) for f in file_list]
    df = pd.concat(all_dfs, ignore_index=True)
    
    # –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –≤—Ö–æ–¥–Ω—ã–µ –∫–ª—é—á–∏, —á—Ç–æ –∏ —É –º–æ–¥–µ–ª–∏
    input_keys = list(full_model.input.keys())
    X = df_to_model_inputs(df, input_keys)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É (–ø–æ–¥—Å–µ—Ç—å)
    # –î–ª—è —ç—Ç–æ–≥–æ –∏–∑–≤–ª–µ—á—ë–º –ø–æ–¥—Å–µ—Ç—å –∏–∑ –º–æ–¥–µ–ª–∏.
    # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –Ω–∞ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–æ–º —É—Ä–æ–≤–Ω–µ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç —á–∞—Å—Ç—å —Å –∏–º–µ–Ω–µ–º "concatenated_features"
    preprocessor_model = models.Model(inputs=full_model.input,
                                      outputs=full_model.get_layer("concatenated_features").output,
                                      name="preprocessor")
    y = preprocessor_model.predict(X)
    
    full_model.fit(X, y, epochs=epochs, batch_size=32, shuffle=True, verbose=1)
    full_model.save(MODEL_PATH)
    print(f"\nüîÑ –î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")

def validate_file(file_path, threshold=0.01):
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    
    full_model = tf.keras.models.load_model(MODEL_PATH, compile=True)
    
    df = load_csv_clean(file_path)
    input_keys = list(full_model.input.keys())
    X = df_to_model_inputs(df, input_keys)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    preprocessor_model = models.Model(inputs=full_model.input,
                                      outputs=full_model.get_layer("concatenated_features").output,
                                      name="preprocessor")
    y_true = preprocessor_model.predict(X)
    
    y_pred = full_model.predict(X)
    # –í—ã—á–∏—Å–ª—è–µ–º –æ—à–∏–±–∫—É —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ (MSE –ø–æ —Å—Ç—Ä–æ–∫–∞–º)
    loss = np.mean(np.square(y_true - y_pred), axis=1)
    
    df["ValidationLoss"] = loss
    df["IsValid"] = df["ValidationLoss"] < threshold
    
    print(df[["ValidationLoss", "IsValid"]].head())
    print(f"\n‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {(df['IsValid']).sum()} –∏–∑ {len(df)}")
    
    return df

# === –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ===
if __name__ == "__main__":
    # –î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—É—á–µ–±–Ω—ã–π –Ω–∞–±–æ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã–µ –∑–∞–ø–∏—Å–∏)
    # train_model_on_files(["data1.csv", "data2.csv"])
    
    # –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    # continue_training(["data3.csv"])
    
    # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ (–Ω–∞ –≤—Ö–æ–¥ –º–æ–∂–Ω–æ –ø–æ–¥–∞–≤–∞—Ç—å –ª—é–±—ã–µ –¥–∞–Ω–Ω—ã–µ)
    # validate_file("unseen_data.csv", threshold=0.02)
    pass
